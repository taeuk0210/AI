{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21726,"status":"ok","timestamp":1678257799619,"user":{"displayName":"강태욱","userId":"11730563988901671307"},"user_tz":-540},"id":"BEe2TOKXiRUn","outputId":"ea043e72-780c-4be4-a64b-988cef1f969a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/esrgan\n","\n","import os\n","import glob\n","import random\n","from PIL import Image\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import torchvision\n","from torchvision.models import vgg19\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image, make_grid\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"CPrQjCKjMy1Y"},"source":["## dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1678257799620,"user":{"displayName":"강태욱","userId":"11730563988901671307"},"user_tz":-540},"id":"sZYMuZ7NMyRb"},"outputs":[],"source":["# pre-trained pytorch model의 normalization parameters\n","\n","mean = np.array([0.485, 0.456, 0.406])\n","std = np.array([0.229, 0.224, 0.225])\n","\n","def denormalize(tensors):\n","  \"\"\" Denormalizes image tensors using mean and std \"\"\"\n","  for c in range(3):\n","    tensors[:, c].mul_(std[c]).add_(mean[c])\n","  return torch.clamp(tensors, 0, 255)\n","\n","\n","class ImageDataset(torch.utils.data.Dataset):\n","  def __init__(self, root, hr_shape):\n","    hr_height, hr_width = hr_shape\n","    # Transforms for low resolution images and high resolution images\n","    self.lr_transform = transforms.Compose(\n","        [transforms.Resize((hr_height // 4, hr_height // 4),\n","                           transforms.InterpolationMode(\"bicubic\")),\n","         transforms.ToTensor(),\n","         transforms.Normalize(mean, std)])\n","    \n","    self.hr_transform = transforms.Compose(\n","        [transforms.Resize((hr_height, hr_height),\n","                           transforms.InterpolationMode(\"bicubic\")),\n","         transforms.ToTensor(),\n","         transforms.Normalize(mean, std)])\n","\n","    self.files = sorted(glob.glob(root + \"/*.jpg\"))\n","\n","  def __getitem__(self, index):\n","      img = Image.open(self.files[index % len(self.files)])\n","      img_lr = self.lr_transform(img)\n","      img_hr = self.hr_transform(img)\n","\n","      return {\"lr\": img_lr, \"hr\": img_hr}\n","\n","  def __len__(self):\n","      return len(self.files)"]},{"cell_type":"markdown","metadata":{"id":"s4FHkTjOMw65"},"source":["## model"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1678257799620,"user":{"displayName":"강태욱","userId":"11730563988901671307"},"user_tz":-540},"id":"PnJBPmZH4MvJ"},"outputs":[],"source":["# Feature Extractor\n","\n","class Feature_Extractor(nn.Module):\n","  def __init__(self):\n","    super(Feature_Extractor, self).__init__()\n","    vgg_model = vgg19(weights=torchvision.models.VGG19_Weights.DEFAULT)\n","    self.vgg19_54 = nn.Sequential(*list(vgg_model.features.children())[:35])\n","\n","  def forward(self, img):\n","    return self.vgg19_54(img)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1678257799621,"user":{"displayName":"강태욱","userId":"11730563988901671307"},"user_tz":-540},"id":"1Q-GrunS40us"},"outputs":[],"source":["# Dense Residual Block & RRDB\n","\n","class Dense_Residual_Block(nn.Module):\n","  def __init__(self, filters, res_scale=0.2):\n","    super(Dense_Residual_Block, self).__init__()\n","    self.res_scale = res_scale\n","\n","    def block(in_features, non_linearity=True):\n","      layers = [nn.Conv2d(in_features, filters,3,1,1,bias=True)]\n","      if non_linearity:\n","        layers += [nn.LeakyReLU()]\n","      return nn.Sequential(*layers)\n","\n","    self.b1 = block(1*filters)\n","    self.b2 = block(2*filters)\n","    self.b3 = block(3*filters)\n","    self.b4 = block(4*filters)\n","    self.b5 = block(5*filters)\n","    self.blocks = [self.b1,self.b2,self.b3,self.b4,self.b5]\n","  \n","  def forward(self, x):\n","    inputs = x\n","    for b in self.blocks:\n","      out = b(inputs)\n","      inputs = torch.cat([inputs, out], dim=1)\n","    return out.mul(self.res_scale) + x\n","\n","class RRDB(nn.Module):\n","  def __init__(self, filters, res_scale=0.2):\n","    super(RRDB, self).__init__()\n","    self.res_scale = res_scale\n","    self.dense_blocks = nn.Sequential(Dense_Residual_Block(filters),\n","                                    Dense_Residual_Block(filters),\n","                                    Dense_Residual_Block(filters))\n","  def forward(self, x):\n","    return self.dense_blocks(x).mul(self.res_scale)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1678257799622,"user":{"displayName":"강태욱","userId":"11730563988901671307"},"user_tz":-540},"id":"M8uDxLS15a99"},"outputs":[],"source":["# Generator & Discriminator\n","\n","class GeneratorRRDB(nn.Module):\n","  def __init__(self, channels, filters=64, num_res_blocks=16, num_upsample=2):\n","    super(GeneratorRRDB, self).__init__()\n","\n","    # First layer\n","    self.conv1 = nn.Conv2d(channels, filters, kernel_size=3, stride=1, padding=1)\n","    # Residual blocks\n","    self.res_blocks = nn.Sequential(*[RRDB(filters) for _ in range(num_res_blocks)])\n","    # Second conv layer post residual blocks\n","    self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n","    # Upsampling layers\n","    upsample_layers = []\n","    for _ in range(num_upsample):\n","        upsample_layers += [\n","            nn.Conv2d(filters, filters * 4, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(),\n","            nn.PixelShuffle(upscale_factor=2)\n","        ]\n","    self.upsampling = nn.Sequential(*upsample_layers)\n","    # Final output block\n","    self.conv3 = nn.Sequential(\n","        nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n","        nn.LeakyReLU(),\n","        nn.Conv2d(filters, channels, kernel_size=3, stride=1, padding=1)\n","    )\n","\n","  def forward(self, x):\n","      out1 = self.conv1(x)\n","      out = self.res_blocks(out1)\n","      out2 = self.conv2(out)\n","      out = torch.add(out1, out2)\n","      out = self.upsampling(out)\n","      out = self.conv3(out)\n","      return out\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, input_shape):\n","        super(Discriminator, self).__init__()\n","\n","        self.input_shape = input_shape\n","        in_channels, in_height, in_width = self.input_shape\n","        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n","        self.output_shape = (1, patch_h, patch_w)\n","\n","        def discriminator_block(in_filters, out_filters, first_block=False):\n","            layers = []\n","            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n","            if not first_block:\n","                layers.append(nn.BatchNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n","            layers.append(nn.BatchNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        layers = []\n","        in_filters = in_channels\n","        for i, out_filters in enumerate([64, 128, 256, 512]):\n","            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n","            in_filters = out_filters\n","\n","        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, img):\n","        return self.model(img)"]},{"cell_type":"markdown","metadata":{"id":"n0WPju3nhONy"},"source":["## training"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1678257799622,"user":{"displayName":"강태욱","userId":"11730563988901671307"},"user_tz":-540},"id":"KQo_k0Dshg-i"},"outputs":[],"source":["# parameters\n","\n","epoch = 0 #epoch to start training from\n","n_epochs = 200 #number of epochs of training\n","dataset_name = \"img_align_celeba\" #name of the dataset\n","batch_size = 64 #size of the batches\n","lr = 0.0002 #adam: learning rate\n","b1 = 0.9 #adam: decay of first order momentum of gradient\n","b2 = 0.999 #adam: decay of first order momentum of gradient\n","decay_epoch = 100 #epoch from which to start lr decay\n","n_cpu = 2 #number of cpu threads to use during batch generation\n","hr_height = 256 #high res. image height\n","hr_width = 256 #high res. image width\n","channels = 3 #number of image channels\n","sample_interval = 100 #interval between saving image samples\n","checkpoint_interval = 5000 #batch interval between model checkpoints\n","residual_blocks = 23 #number of residual blocks in the generator\n","warmup_batches = 500 #number of batches with pixel-wise loss only\n","lambda_adv = 5e-3 #adversarial loss weight\n","lambda_pixel = 1e-2 #pixel-wise loss weight"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["73c682abaa8e4a709748f060b6c089db","daa689ecd2124485a5d7506edc9c0c04","f16852e63139475b993e65dde40c4de2","f47ef1cff53b4d138439879cf064f689","51189c5f52c64b8b904c412428d6018c","1868af01934a407aad7fd690f8a4aae7","01514e43da0e4fc8ae37ca541732f1d8","4fd214d17cdf470ba6cc98d6250fe968","90a42a9d59384275be85876a6c5896ba","f10c48919ce14e20aa7560ebd7e16372","edc48af6dbce4e689a6cb61c49d6a2ae"]},"executionInfo":{"elapsed":5048,"status":"ok","timestamp":1678257805051,"user":{"displayName":"강태욱","userId":"11730563988901671307"},"user_tz":-540},"id":"gC4pnyHp8qG0","outputId":"684565f7-b5c7-4c20-f603-2d5c95053ac8"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73c682abaa8e4a709748f060b6c089db"}},"metadata":{}}],"source":["# prepare training\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","hr_shape = (hr_height, hr_width)\n","\n","# Initialize generator and discriminator\n","generator = GeneratorRRDB(channels, filters=64, num_res_blocks=residual_blocks).to(device)\n","discriminator = Discriminator(input_shape=(channels, *hr_shape)).to(device)\n","feature_extractor = Feature_Extractor().to(device)\n","\n","# Set feature extractor to inference mode\n","feature_extractor.eval()\n","\n","# Losses\n","criterion_GAN = nn.BCEWithLogitsLoss().to(device)\n","criterion_content = nn.L1Loss().to(device)\n","criterion_pixel = nn.L1Loss().to(device)\n","\n","# if epoch != 0:\n","#     # Load pretrained models\n","#     generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\" % epoch))\n","#     discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\" % epoch))\n","\n","# optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n","\n","Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"n21r_lC9sMvt","executionInfo":{"status":"error","timestamp":1678257925765,"user_tz":-540,"elapsed":120717,"user":{"displayName":"강태욱","userId":"11730563988901671307"}},"outputId":"6881bbb8-5120-42f6-e4b6-e83b090c9d61"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-775e3e27d881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Datasets/CelebA/img_align_celeba'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m dataloader = DataLoader(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhr_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    108\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}],"source":["root = '/content/drive/MyDrive/Colab Notebooks/Datasets/CelebA/img_align_celeba'\n","\n","dataloader = DataLoader(\n","    ImageDataset(root, hr_shape=hr_shape),\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=n_cpu)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaEMpf5ttBJA","executionInfo":{"status":"aborted","timestamp":1678257925769,"user_tz":-540,"elapsed":16,"user":{"displayName":"강태욱","userId":"11730563988901671307"}}},"outputs":[],"source":["# imgs = next(iter(dataloader))\n","# imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n","# gen_hr = generator(imgs_lr)\n","\n","# save_path = '/content/drive/MyDrive/Colab Notebooks/코드 이론/model_save/ESRGAN'\n","# imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n","# img_grid = denormalize(torch.cat((imgs_lr, gen_hr), -1))\n","\n","# save_image(img_grid, save_path+\"/images/test.png\", nrow=1, normalize=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1678257925771,"user":{"displayName":"강태욱","userId":"11730563988901671307"},"user_tz":-540},"id":"fS0SfDgck8bV"},"outputs":[],"source":["# training\n","\n","save_path = '/content/drive/MyDrive/Colab Notebooks/코드 이론/model_save/ESRGAN'\n","\n","for epoch in range(epoch, n_epochs):\n","  for i, imgs in enumerate(dataloader):\n","\n","    batches_done = epoch * len(dataloader) + i\n","\n","    # model input\n","    imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n","    imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n","\n","    # Adversarial ground truths\n","    valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n","    fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n","\n","    # ------------------\n","    #  Train Generators\n","    # ------------------\n","\n","    optimizer_G.zero_grad()\n","\n","    # Generate a high resolution image from low resolution input\n","    gen_hr = generator(imgs_lr)\n","\n","    # Measure pixel-wise loss against ground truth\n","    loss_pixel = criterion_pixel(gen_hr, imgs_hr)\n","\n","    if batches_done < warmup_batches:\n","        # Warm-up (pixel-wise loss only)\n","        loss_pixel.backward()\n","        optimizer_G.step()\n","        print(\n","            \"[Epoch %d/%d] [Batch %d/%d] [G pixel: %f]\"\n","            % (epoch, n_epochs, i, len(dataloader), loss_pixel.item())\n","        )\n","        continue\n","\n","    # Extract validity predictions from discriminator\n","    pred_real = discriminator(imgs_hr).detach()\n","    pred_fake = discriminator(gen_hr)\n","\n","    # Adversarial loss (relativistic average GAN)\n","    loss_GAN = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n","\n","    # Content loss\n","    gen_features = feature_extractor(gen_hr)\n","    real_features = feature_extractor(imgs_hr).detach()\n","    loss_content = criterion_content(gen_features, real_features)\n","\n","    # Total generator loss\n","    loss_G = loss_content + lambda_adv * loss_GAN + lambda_pixel * loss_pixel\n","\n","    loss_G.backward()\n","    optimizer_G.step()\n","\n","    # ---------------------\n","    #  Train Discriminator\n","    # ---------------------\n","\n","    optimizer_D.zero_grad()\n","\n","    pred_real = discriminator(imgs_hr)\n","    pred_fake = discriminator(gen_hr.detach())\n","\n","    # Adversarial loss for real and fake images (relativistic average GAN)\n","    loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n","    loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n","\n","    # Total loss\n","    loss_D = (loss_real + loss_fake) / 2\n","\n","    loss_D.backward()\n","    optimizer_D.step()\n","\n","    # --------------\n","    #  Log Progress\n","    # --------------\n","    if i%200==0:\n","      print(\n","        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, content: %f, adv: %f, pixel: %f]\"\n","        % (\n","            epoch,\n","            n_epochs,\n","            i,\n","            len(dataloader),\n","            loss_D.item(),\n","            loss_G.item(),\n","            loss_content.item(),\n","            loss_GAN.item(),\n","            loss_pixel.item(),\n","        )\n","    )\n","    \n","    if batches_done % sample_interval == 0:\n","        # Save image grid with upsampled inputs and ESRGAN outputs\n","        imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n","        img_grid = denormalize(torch.cat((imgs_lr, gen_hr,imgs_hr), -1))\n","        save_image(img_grid, save_path+\"/images/%d.png\" % batches_done, nrow=1, normalize=False)\n","\n","    if batches_done % checkpoint_interval == 0:\n","        # Save model checkpoints\n","        torch.save(generator.state_dict(), save_path+\"/generator_%d.pth\" % epoch)\n","        torch.save(discriminator.state_dict(), save_path+\"/discriminator_%d.pth\" %epoch)"]},{"cell_type":"markdown","metadata":{"id":"L9GbCI2-8usE"},"source":["## Note\n","\n","https://arxiv.org/pdf/1902.06068.pdf  \n","\n","생각해볼 것들\n","1. performance evaluation\n"," - PSNR, SSIM, LPIPS 등이 있는데 다른 measurement를 만들자\n","2. network design & learning strategies\n"," - 다른 모델들 mix or objective 추가\n","3. upsampling methods\n"," - cnn 기반의 sota들은 보통 upsampling으로 bicubic, deconv, sub-pixel 를 이용 다른걸 찾거나 만들어보기\n","4. unsupervised SR\n"," - 이건 뭘해야할지 잘 모르겠음... ill-posed 때문에 supervised를 하는데 비지도로 바꾸려면 새로운 방식이 필요할듯\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dDQdypnyoTI","executionInfo":{"status":"aborted","timestamp":1678257925772,"user_tz":-540,"elapsed":17,"user":{"displayName":"강태욱","userId":"11730563988901671307"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMhfWP517j51s2fltJ3Xu8B"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"73c682abaa8e4a709748f060b6c089db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_daa689ecd2124485a5d7506edc9c0c04","IPY_MODEL_f16852e63139475b993e65dde40c4de2","IPY_MODEL_f47ef1cff53b4d138439879cf064f689"],"layout":"IPY_MODEL_51189c5f52c64b8b904c412428d6018c"}},"daa689ecd2124485a5d7506edc9c0c04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1868af01934a407aad7fd690f8a4aae7","placeholder":"​","style":"IPY_MODEL_01514e43da0e4fc8ae37ca541732f1d8","value":"100%"}},"f16852e63139475b993e65dde40c4de2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fd214d17cdf470ba6cc98d6250fe968","max":574673361,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90a42a9d59384275be85876a6c5896ba","value":574673361}},"f47ef1cff53b4d138439879cf064f689":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f10c48919ce14e20aa7560ebd7e16372","placeholder":"​","style":"IPY_MODEL_edc48af6dbce4e689a6cb61c49d6a2ae","value":" 548M/548M [00:02&lt;00:00, 228MB/s]"}},"51189c5f52c64b8b904c412428d6018c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1868af01934a407aad7fd690f8a4aae7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01514e43da0e4fc8ae37ca541732f1d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fd214d17cdf470ba6cc98d6250fe968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90a42a9d59384275be85876a6c5896ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f10c48919ce14e20aa7560ebd7e16372":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edc48af6dbce4e689a6cb61c49d6a2ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}