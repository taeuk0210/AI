{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMpEgO8b4mmd+TtkmxuoFVI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import argparse\n","import os\n","import numpy as np\n","\n","from torch.autograd import Variable\n","from torch.autograd import grad as torch_grad\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from itertools import chain as ichain"],"metadata":{"id":"_AUneLer7ki4","executionInfo":{"status":"ok","timestamp":1692720126608,"user_tz":-540,"elapsed":9,"user":{"displayName":"강태욱","userId":"11730563988901671307"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"GJ9KJYiUR6y9","executionInfo":{"status":"ok","timestamp":1692720126608,"user_tz":-540,"elapsed":7,"user":{"displayName":"강태욱","userId":"11730563988901671307"}}},"outputs":[],"source":["# Sample a random latent space vector\n","def sample_z(shape=64, latent_dim=10, n_c=10, fix_class=-1, req_grad=False):\n","\n","    assert (fix_class == -1 or (fix_class >= 0 and fix_class < n_c) ), \"Requested class %i outside bounds.\"%fix_class\n","\n","    Tensor = torch.cuda.FloatTensor\n","\n","    # Sample noise as generator input, zn\n","    zn = Variable(Tensor(0.75*np.random.normal(0, 1, (shape, latent_dim))), requires_grad=req_grad)\n","\n","    ######### zc, zc_idx variables with grads, and zc to one-hot vector\n","    # Pure one-hot vector generation\n","    zc_FT = Tensor(shape, n_c).fill_(0)\n","    zc_idx = torch.empty(shape, dtype=torch.long)\n","\n","    if (fix_class == -1):\n","        zc_idx = zc_idx.random_(n_c).cuda()\n","        zc_FT = zc_FT.scatter_(1, zc_idx.unsqueeze(1), 1.)\n","    else:\n","        zc_idx[:] = fix_class\n","        zc_FT[:, fix_class] = 1\n","\n","        zc_idx = zc_idx.cuda()\n","        zc_FT = zc_FT.cuda()\n","\n","    zc = Variable(zc_FT, requires_grad=req_grad)\n","\n","    # Return components of latent space variable\n","    return zn, zc, zc_idx\n","\n","def calc_gradient_penalty(netD, real_data, generated_data):\n","    # GP strength\n","    LAMBDA = 10\n","\n","    b_size = real_data.size()[0]\n","\n","    # Calculate interpolation\n","    alpha = torch.rand(b_size, 1, 1, 1)\n","    alpha = alpha.expand_as(real_data)\n","    alpha = alpha.cuda()\n","\n","    interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n","    interpolated = Variable(interpolated, requires_grad=True)\n","    interpolated = interpolated.cuda()\n","\n","    # Calculate probability of interpolated examples\n","    prob_interpolated = netD(interpolated)\n","\n","    # Calculate gradients of probabilities with respect to examples\n","    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n","                           grad_outputs=torch.ones(prob_interpolated.size()).cuda(),\n","                           create_graph=True, retain_graph=True)[0]\n","\n","    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n","    # so flatten to easily take norm per example in batch\n","    gradients = gradients.view(b_size, -1)\n","\n","    # Derivatives of the gradient close to 0 can cause problems because of\n","    # the square root, so manually calculate norm and add epsilon\n","    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n","\n","    # Return gradient penalty\n","    return LAMBDA * ((gradients_norm - 1) ** 2).mean()\n","\n","\n","# Weight Initializer\n","def initialize_weights(net):\n","    for m in net.modules():\n","        if isinstance(m, nn.Conv2d):\n","            m.weight.data.normal_(0, 0.02)\n","            m.bias.data.zero_()\n","        elif isinstance(m, nn.ConvTranspose2d):\n","            m.weight.data.normal_(0, 0.02)\n","            m.bias.data.zero_()\n","        elif isinstance(m, nn.Linear):\n","            m.weight.data.normal_(0, 0.02)\n","            m.bias.data.zero_()\n","\n","\n","# Softmax function\n","def softmax(x):\n","    return F.softmax(x, dim=1)\n","\n","\n","class Reshape(nn.Module):\n","    \"\"\"\n","    Class for performing a reshape as a layer in a sequential model.\n","    \"\"\"\n","    def __init__(self, shape=[]):\n","        super(Reshape, self).__init__()\n","        self.shape = shape\n","\n","    def forward(self, x):\n","        return x.view(x.size(0), *self.shape)\n","\n","    def extra_repr(self):\n","            # (Optional)Set the extra information about this module. You can test\n","            # it by printing an object of this class.\n","            return 'shape={}'.format(\n","                self.shape\n","            )\n","\n","\n","class Generator_CNN(nn.Module):\n","    \"\"\"\n","    CNN to model the generator of a ClusterGAN\n","    Input is a vector from representation space of dimension z_dim\n","    output is a vector from image space of dimension X_dim\n","    \"\"\"\n","    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n","    def __init__(self, latent_dim, n_c, x_shape, verbose=False):\n","        super(Generator_CNN, self).__init__()\n","\n","        self.name = 'generator'\n","        self.latent_dim = latent_dim\n","        self.n_c = n_c\n","        self.x_shape = x_shape\n","        self.ishape = (128, 7, 7)\n","        self.iels = int(np.prod(self.ishape))\n","        self.verbose = verbose\n","\n","        self.model = nn.Sequential(\n","            # Fully connected layers\n","            torch.nn.Linear(self.latent_dim + self.n_c, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            torch.nn.Linear(1024, self.iels),\n","            nn.BatchNorm1d(self.iels),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # Reshape to 128 x (7x7)\n","            Reshape(self.ishape),\n","\n","            # Upconvolution layers\n","            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=True),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=True),\n","            nn.Sigmoid()\n","        )\n","\n","        initialize_weights(self)\n","\n","        if self.verbose:\n","            print(\"Setting up {}...\\n\".format(self.name))\n","            print(self.model)\n","\n","    def forward(self, zn, zc):\n","        z = torch.cat((zn, zc), 1)\n","        x_gen = self.model(z)\n","        # Reshape for output\n","        x_gen = x_gen.view(x_gen.size(0), *self.x_shape)\n","        return x_gen\n","\n","\n","class Encoder_CNN(nn.Module):\n","    \"\"\"\n","    CNN to model the encoder of a ClusterGAN\n","    Input is vector X from image space if dimension X_dim\n","    Output is vector z from representation space of dimension z_dim\n","    \"\"\"\n","    def __init__(self, latent_dim, n_c, verbose=False):\n","        super(Encoder_CNN, self).__init__()\n","\n","        self.name = 'encoder'\n","        self.channels = 1\n","        self.latent_dim = latent_dim\n","        self.n_c = n_c\n","        self.cshape = (128, 5, 5)\n","        self.iels = int(np.prod(self.cshape))\n","        self.lshape = (self.iels,)\n","        self.verbose = verbose\n","\n","        self.model = nn.Sequential(\n","            # Convolutional layers\n","            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # Flatten\n","            Reshape(self.lshape),\n","\n","            # Fully connected layers\n","            torch.nn.Linear(self.iels, 1024),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            torch.nn.Linear(1024, latent_dim + n_c)\n","        )\n","\n","        initialize_weights(self)\n","\n","        if self.verbose:\n","            print(\"Setting up {}...\\n\".format(self.name))\n","            print(self.model)\n","\n","    def forward(self, in_feat):\n","        z_img = self.model(in_feat)\n","        # Reshape for output\n","        z = z_img.view(z_img.shape[0], -1)\n","        # Separate continuous and one-hot components\n","        zn = z[:, 0:self.latent_dim]\n","        zc_logits = z[:, self.latent_dim:]\n","        # Softmax on zc component\n","        zc = softmax(zc_logits)\n","        return zn, zc, zc_logits\n","\n","\n","class Discriminator_CNN(nn.Module):\n","    \"\"\"\n","    CNN to model the discriminator of a ClusterGAN\n","    Input is tuple (X,z) of an image vector and its corresponding\n","    representation z vector. For example, if X comes from the dataset, corresponding\n","    z is Encoder(X), and if z is sampled from representation space, X is Generator(z)\n","    Output is a 1-dimensional value\n","    \"\"\"\n","    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n","    def __init__(self, wass_metric=False, verbose=False):\n","        super(Discriminator_CNN, self).__init__()\n","\n","        self.name = 'discriminator'\n","        self.channels = 1\n","        self.cshape = (128, 5, 5)\n","        self.iels = int(np.prod(self.cshape))\n","        self.lshape = (self.iels,)\n","        self.wass = wass_metric\n","        self.verbose = verbose\n","\n","        self.model = nn.Sequential(\n","            # Convolutional layers\n","            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # Flatten\n","            Reshape(self.lshape),\n","\n","            # Fully connected layers\n","            torch.nn.Linear(self.iels, 1024),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            torch.nn.Linear(1024, 1),\n","        )\n","\n","        # If NOT using Wasserstein metric, final Sigmoid\n","        if (not self.wass):\n","            self.model = nn.Sequential(self.model, torch.nn.Sigmoid())\n","\n","        initialize_weights(self)\n","\n","        if self.verbose:\n","            print(\"Setting up {}...\\n\".format(self.name))\n","            print(self.model)\n","\n","    def forward(self, img):\n","        # Get output\n","        validity = self.model(img)\n","        return validity\n"]},{"cell_type":"code","source":["\n","# Training details\n","n_epochs = 200\n","batch_size = 64\n","test_batch_size = 5000\n","lr = 0.0001\n","b1 = 0.5\n","b2 = 0.9\n","decay = 2.5*1e-5\n","n_skip_iter = 5\n","\n","# Data dimensions\n","img_size = 28\n","channels = 1\n","\n","# Latent space info\n","latent_dim = 30\n","n_c = 10\n","betan = 10\n","betac = 10\n","\n","# Wasserstein+GP metric flag\n","wass_metric = 'store_true'\n","\n","x_shape = (channels, img_size, img_size)\n","\n","cuda = True if torch.cuda.is_available() else False\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# Loss function\n","bce_loss = torch.nn.BCELoss()\n","xe_loss = torch.nn.CrossEntropyLoss()\n","mse_loss = torch.nn.MSELoss()\n","\n","# Initialize generator and discriminator\n","generator = Generator_CNN(latent_dim, n_c, x_shape)\n","encoder = Encoder_CNN(latent_dim, n_c)\n","discriminator = Discriminator_CNN(wass_metric=wass_metric)\n","\n","if cuda:\n","    generator.cuda()\n","    encoder.cuda()\n","    discriminator.cuda()\n","    bce_loss.cuda()\n","    xe_loss.cuda()\n","    mse_loss.cuda()\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","# Configure data loader\n","\n","dataloader = torch.utils.data.DataLoader(\n","    datasets.MNIST(\n","        \"./\",\n","        train=True,\n","        download=True,\n","        transform=transforms.Compose(\n","            [transforms.ToTensor()]\n","        ),\n","    ),\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","\n","# Test data loader\n","testdata = torch.utils.data.DataLoader(\n","    datasets.MNIST(\n","        \"./\",\n","        train=False,\n","        download=True,\n","        transform=transforms.Compose(\n","            [transforms.ToTensor()]\n","        ),\n","    ),\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","test_imgs, test_labels = next(iter(testdata))\n","test_imgs = Variable(test_imgs.type(Tensor))\n","\n","ge_chain = ichain(generator.parameters(),\n","                  encoder.parameters())\n","\n","optimizer_GE = torch.optim.Adam(ge_chain, lr=lr, betas=(b1, b2), weight_decay=decay)\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n","\n","# ----------\n","#  Training\n","# ----------\n","ge_l = []\n","d_l = []\n","\n","c_zn = []\n","c_zc = []\n","c_i = []\n","\n","# Training loop\n","print('\\nBegin training session with %i epochs...\\n'%(n_epochs))\n","for epoch in range(n_epochs):\n","    for i, (imgs, itruth_label) in enumerate(dataloader):\n","\n","        # Ensure generator/encoder are trainable\n","        generator.train()\n","        encoder.train()\n","\n","        # Zero gradients for models\n","        generator.zero_grad()\n","        encoder.zero_grad()\n","        discriminator.zero_grad()\n","\n","        # Configure input\n","        real_imgs = Variable(imgs.type(Tensor))\n","\n","        # ---------------------------\n","        #  Train Generator + Encoder\n","        # ---------------------------\n","\n","        optimizer_GE.zero_grad()\n","\n","        # Sample random latent variables\n","        zn, zc, zc_idx = sample_z(shape=imgs.shape[0],\n","                                  latent_dim=latent_dim,\n","                                  n_c=n_c)\n","\n","        # Generate a batch of images\n","        gen_imgs = generator(zn, zc)\n","\n","        # Discriminator output from real and generated samples\n","        D_gen = discriminator(gen_imgs)\n","        D_real = discriminator(real_imgs)\n","\n","        # Step for Generator & Encoder, n_skip_iter times less than for discriminator\n","        if (i % n_skip_iter == 0):\n","            # Encode the generated images\n","            enc_gen_zn, enc_gen_zc, enc_gen_zc_logits = encoder(gen_imgs)\n","\n","            # Calculate losses for z_n, z_c\n","            zn_loss = mse_loss(enc_gen_zn, zn)\n","            zc_loss = xe_loss(enc_gen_zc_logits, zc_idx)\n","\n","            # Check requested metric\n","            if wass_metric:\n","                # Wasserstein GAN loss\n","                ge_loss = torch.mean(D_gen) + betan * zn_loss + betac * zc_loss\n","            else:\n","                # Vanilla GAN loss\n","                valid = Variable(Tensor(gen_imgs.size(0), 1).fill_(1.0), requires_grad=False)\n","                v_loss = bce_loss(D_gen, valid)\n","                ge_loss = v_loss + betan * zn_loss + betac * zc_loss\n","\n","            ge_loss.backward(retain_graph=True)\n","            optimizer_GE.step()\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","\n","        # Measure discriminator's ability to classify real from generated samples\n","        if wass_metric:\n","            # Gradient penalty term\n","            grad_penalty = calc_gradient_penalty(discriminator, real_imgs, gen_imgs)\n","\n","            # Wasserstein GAN loss w/gradient penalty\n","            d_loss = torch.mean(D_real) - torch.mean(D_gen) + grad_penalty\n","\n","        else:\n","            # Vanilla GAN loss\n","            fake = Variable(Tensor(gen_imgs.size(0), 1).fill_(0.0), requires_grad=False)\n","            real_loss = bce_loss(D_real, valid)\n","            fake_loss = bce_loss(D_gen, fake)\n","            d_loss = (real_loss + fake_loss) / 2\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","\n","    # Save training losses\n","    d_l.append(d_loss.item())\n","    ge_l.append(ge_loss.item())\n","\n","\n","    # Generator in eval mode\n","    generator.eval()\n","    encoder.eval()\n","\n","    # Set number of examples for cycle calcs\n","    n_sqrt_samp = 5\n","    n_samp = n_sqrt_samp * n_sqrt_samp\n","\n","\n","    ## Cycle through test real -> enc -> gen\n","    t_imgs, t_label = test_imgs.data, test_labels\n","    # Encode sample real instances\n","    e_tzn, e_tzc, e_tzc_logits = encoder(t_imgs)\n","    # Generate sample instances from encoding\n","    teg_imgs = generator(e_tzn, e_tzc)\n","    # Calculate cycle reconstruction loss\n","    img_mse_loss = mse_loss(t_imgs, teg_imgs)\n","    # Save img reco cycle loss\n","    c_i.append(img_mse_loss.item())\n","\n","\n","    ## Cycle through randomly sampled encoding -> generator -> encoder\n","    zn_samp, zc_samp, zc_samp_idx = sample_z(shape=n_samp,\n","                                             latent_dim=latent_dim,\n","                                             n_c=n_c)\n","    # Generate sample instances\n","    gen_imgs_samp = generator(zn_samp, zc_samp)\n","\n","    # Encode sample instances\n","    zn_e, zc_e, zc_e_logits = encoder(gen_imgs_samp)\n","\n","    # Calculate cycle latent losses\n","    lat_mse_loss = mse_loss(zn_e, zn_samp)\n","    lat_xe_loss = xe_loss(zc_e_logits, zc_samp_idx)\n","\n","    # Save latent space cycle losses\n","    c_zn.append(lat_mse_loss.item())\n","    c_zc.append(lat_xe_loss.item())\n","\n","    # Save cycled and generated examples!\n","    r_imgs, i_label = real_imgs.data[:n_samp], itruth_label[:n_samp]\n","    e_zn, e_zc, e_zc_logits = encoder(r_imgs)\n","    reg_imgs = generator(e_zn, e_zc)\n","\n","    save_image(reg_imgs.data[:n_samp],\n","               './images/cycle_reg_%06i.png' %(epoch),\n","               nrow=n_sqrt_samp, normalize=True)\n","    save_image(gen_imgs_samp.data[:n_samp],\n","               './images/gen_%06i.png' %(epoch),\n","               nrow=n_sqrt_samp, normalize=True)\n","\n","    ## Generate samples for specified classes\n","    stack_imgs = []\n","    for idx in range(n_c):\n","        # Sample specific class\n","        zn_samp, zc_samp, zc_samp_idx = sample_z(shape=n_c,\n","                                                 latent_dim=latent_dim,\n","                                                 n_c=n_c,\n","                                                 fix_class=idx)\n","\n","        # Generate sample instances\n","        gen_imgs_samp = generator(zn_samp, zc_samp)\n","\n","        if (len(stack_imgs) == 0):\n","            stack_imgs = gen_imgs_samp\n","        else:\n","            stack_imgs = torch.cat((stack_imgs, gen_imgs_samp), 0)\n","\n","    # Save class-specified generated examples!\n","    save_image(stack_imgs,\n","               './images/gen_classes_%06i.png' %(epoch),\n","               nrow=n_c, normalize=True)\n","\n","\n","    print (\"[Epoch %d/%d] \\n\"\\\n","           \"\\tModel Losses: [D: %f] [GE: %f]\" % (epoch,\n","                                                 n_epochs,\n","                                                 d_loss.item(),\n","                                                 ge_loss.item())\n","          )\n","\n","    print(\"\\tCycle Losses: [x: %f] [z_n: %f] [z_c: %f]\"%(img_mse_loss.item(),\n","                                                         lat_mse_loss.item(),\n","                                                         lat_xe_loss.item())\n","         )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3deIncGl8m2-","executionInfo":{"status":"ok","timestamp":1692724159121,"user_tz":-540,"elapsed":3809105,"user":{"displayName":"강태욱","userId":"11730563988901671307"}},"outputId":"8a32806e-8b21-4612-8c24-461e3749b6ea"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Begin training session with 200 epochs...\n","\n","[Epoch 0/200] \n","\tModel Losses: [D: -6.088988] [GE: -2.231409]\n","\tCycle Losses: [x: 0.079259] [z_n: 0.370109] [z_c: 0.018206]\n","[Epoch 1/200] \n","\tModel Losses: [D: -3.704829] [GE: -11.288058]\n","\tCycle Losses: [x: 0.041655] [z_n: 0.124941] [z_c: 0.002244]\n","[Epoch 2/200] \n","\tModel Losses: [D: -2.714017] [GE: -12.626712]\n","\tCycle Losses: [x: 0.033897] [z_n: 0.096661] [z_c: 0.000969]\n","[Epoch 3/200] \n","\tModel Losses: [D: -2.168938] [GE: -11.795986]\n","\tCycle Losses: [x: 0.029543] [z_n: 0.116829] [z_c: 0.000969]\n","[Epoch 4/200] \n","\tModel Losses: [D: -1.869732] [GE: -11.153236]\n","\tCycle Losses: [x: 0.026933] [z_n: 0.092784] [z_c: 0.001810]\n","[Epoch 5/200] \n","\tModel Losses: [D: -1.716864] [GE: -11.063359]\n","\tCycle Losses: [x: 0.027379] [z_n: 0.104511] [z_c: 0.001580]\n","[Epoch 6/200] \n","\tModel Losses: [D: -1.623061] [GE: -11.638286]\n","\tCycle Losses: [x: 0.024243] [z_n: 0.086401] [z_c: 0.000654]\n","[Epoch 7/200] \n","\tModel Losses: [D: -1.408793] [GE: -11.097169]\n","\tCycle Losses: [x: 0.023633] [z_n: 0.074469] [z_c: 0.000362]\n","[Epoch 8/200] \n","\tModel Losses: [D: -1.143313] [GE: -11.022490]\n","\tCycle Losses: [x: 0.025045] [z_n: 0.076902] [z_c: 0.000464]\n","[Epoch 9/200] \n","\tModel Losses: [D: -1.273185] [GE: -9.903221]\n","\tCycle Losses: [x: 0.024231] [z_n: 0.076636] [z_c: 0.000483]\n","[Epoch 10/200] \n","\tModel Losses: [D: -1.288691] [GE: -6.552702]\n","\tCycle Losses: [x: 0.024527] [z_n: 0.070555] [z_c: 0.000542]\n","[Epoch 11/200] \n","\tModel Losses: [D: -1.077495] [GE: -5.807345]\n","\tCycle Losses: [x: 0.025110] [z_n: 0.074470] [z_c: 0.001552]\n","[Epoch 12/200] \n","\tModel Losses: [D: -0.861110] [GE: -4.028406]\n","\tCycle Losses: [x: 0.026389] [z_n: 0.059221] [z_c: 0.001128]\n","[Epoch 13/200] \n","\tModel Losses: [D: -1.209345] [GE: -4.425615]\n","\tCycle Losses: [x: 0.027021] [z_n: 0.059107] [z_c: 0.000185]\n","[Epoch 14/200] \n","\tModel Losses: [D: -1.260777] [GE: -2.796120]\n","\tCycle Losses: [x: 0.027589] [z_n: 0.050938] [z_c: 0.000334]\n","[Epoch 15/200] \n","\tModel Losses: [D: -0.898478] [GE: -3.479900]\n","\tCycle Losses: [x: 0.028467] [z_n: 0.047358] [z_c: 0.001564]\n","[Epoch 16/200] \n","\tModel Losses: [D: -0.922086] [GE: -2.899415]\n","\tCycle Losses: [x: 0.028690] [z_n: 0.047634] [z_c: 0.000552]\n","[Epoch 17/200] \n","\tModel Losses: [D: -0.886051] [GE: -1.775216]\n","\tCycle Losses: [x: 0.028998] [z_n: 0.045498] [z_c: 0.000226]\n","[Epoch 18/200] \n","\tModel Losses: [D: -0.831560] [GE: -0.471786]\n","\tCycle Losses: [x: 0.029555] [z_n: 0.041599] [z_c: 0.000127]\n","[Epoch 19/200] \n","\tModel Losses: [D: -0.845107] [GE: 0.588839]\n","\tCycle Losses: [x: 0.029093] [z_n: 0.055034] [z_c: 0.000955]\n","[Epoch 20/200] \n","\tModel Losses: [D: -0.887749] [GE: -0.095038]\n","\tCycle Losses: [x: 0.029661] [z_n: 0.035430] [z_c: 0.000127]\n","[Epoch 21/200] \n","\tModel Losses: [D: -0.862831] [GE: 0.811499]\n","\tCycle Losses: [x: 0.029734] [z_n: 0.042079] [z_c: 0.000389]\n","[Epoch 22/200] \n","\tModel Losses: [D: -0.761936] [GE: 0.492862]\n","\tCycle Losses: [x: 0.029733] [z_n: 0.041236] [z_c: 0.000380]\n","[Epoch 23/200] \n","\tModel Losses: [D: -0.987157] [GE: -0.602920]\n","\tCycle Losses: [x: 0.030914] [z_n: 0.034552] [z_c: 0.000312]\n","[Epoch 24/200] \n","\tModel Losses: [D: -0.861394] [GE: -0.297958]\n","\tCycle Losses: [x: 0.031193] [z_n: 0.035941] [z_c: 0.000090]\n","[Epoch 25/200] \n","\tModel Losses: [D: -0.917918] [GE: -0.238727]\n","\tCycle Losses: [x: 0.031207] [z_n: 0.035713] [z_c: 0.000055]\n","[Epoch 26/200] \n","\tModel Losses: [D: -0.878928] [GE: -0.431313]\n","\tCycle Losses: [x: 0.030560] [z_n: 0.035076] [z_c: 0.000064]\n","[Epoch 27/200] \n","\tModel Losses: [D: -0.849548] [GE: -0.357957]\n","\tCycle Losses: [x: 0.030361] [z_n: 0.032663] [z_c: 0.000132]\n","[Epoch 28/200] \n","\tModel Losses: [D: -0.706378] [GE: -0.705154]\n","\tCycle Losses: [x: 0.030053] [z_n: 0.033080] [z_c: 0.000221]\n","[Epoch 29/200] \n","\tModel Losses: [D: -0.700049] [GE: -1.013470]\n","\tCycle Losses: [x: 0.030496] [z_n: 0.032654] [z_c: 0.000122]\n","[Epoch 30/200] \n","\tModel Losses: [D: -0.939972] [GE: -1.217769]\n","\tCycle Losses: [x: 0.031425] [z_n: 0.036307] [z_c: 0.002389]\n","[Epoch 31/200] \n","\tModel Losses: [D: -0.521944] [GE: -1.891952]\n","\tCycle Losses: [x: 0.032364] [z_n: 0.030918] [z_c: 0.000063]\n","[Epoch 32/200] \n","\tModel Losses: [D: -0.613211] [GE: -1.204675]\n","\tCycle Losses: [x: 0.032709] [z_n: 0.032545] [z_c: 0.000367]\n","[Epoch 33/200] \n","\tModel Losses: [D: -0.981867] [GE: -1.335382]\n","\tCycle Losses: [x: 0.031912] [z_n: 0.032322] [z_c: 0.000724]\n","[Epoch 34/200] \n","\tModel Losses: [D: -0.603116] [GE: -1.216851]\n","\tCycle Losses: [x: 0.032693] [z_n: 0.031207] [z_c: 0.000662]\n","[Epoch 35/200] \n","\tModel Losses: [D: -0.736415] [GE: -1.031242]\n","\tCycle Losses: [x: 0.031801] [z_n: 0.036518] [z_c: 0.000145]\n","[Epoch 36/200] \n","\tModel Losses: [D: -0.721570] [GE: -1.733811]\n","\tCycle Losses: [x: 0.033239] [z_n: 0.033955] [z_c: 0.000022]\n","[Epoch 37/200] \n","\tModel Losses: [D: -0.760713] [GE: -1.011569]\n","\tCycle Losses: [x: 0.032211] [z_n: 0.030322] [z_c: 0.000044]\n","[Epoch 38/200] \n","\tModel Losses: [D: -0.756966] [GE: -1.574125]\n","\tCycle Losses: [x: 0.034224] [z_n: 0.028032] [z_c: 0.000476]\n","[Epoch 39/200] \n","\tModel Losses: [D: -0.678988] [GE: -0.952462]\n","\tCycle Losses: [x: 0.032524] [z_n: 0.024526] [z_c: 0.000040]\n","[Epoch 40/200] \n","\tModel Losses: [D: -0.592715] [GE: -1.019460]\n","\tCycle Losses: [x: 0.035102] [z_n: 0.027964] [z_c: 0.000996]\n","[Epoch 41/200] \n","\tModel Losses: [D: -0.659460] [GE: -0.866298]\n","\tCycle Losses: [x: 0.034035] [z_n: 0.027656] [z_c: 0.000161]\n","[Epoch 42/200] \n","\tModel Losses: [D: -0.579987] [GE: -1.963107]\n","\tCycle Losses: [x: 0.033700] [z_n: 0.024960] [z_c: 0.000042]\n","[Epoch 43/200] \n","\tModel Losses: [D: -0.597757] [GE: -1.680168]\n","\tCycle Losses: [x: 0.035352] [z_n: 0.027644] [z_c: 0.000115]\n","[Epoch 44/200] \n","\tModel Losses: [D: -0.668520] [GE: -2.158515]\n","\tCycle Losses: [x: 0.033047] [z_n: 0.030997] [z_c: 0.000175]\n","[Epoch 45/200] \n","\tModel Losses: [D: -0.654567] [GE: -2.253988]\n","\tCycle Losses: [x: 0.034114] [z_n: 0.028852] [z_c: 0.001000]\n","[Epoch 46/200] \n","\tModel Losses: [D: -0.596663] [GE: -1.062361]\n","\tCycle Losses: [x: 0.036211] [z_n: 0.025945] [z_c: 0.000476]\n","[Epoch 47/200] \n","\tModel Losses: [D: -0.790080] [GE: -1.844132]\n","\tCycle Losses: [x: 0.035159] [z_n: 0.026915] [z_c: 0.000089]\n","[Epoch 48/200] \n","\tModel Losses: [D: -0.601537] [GE: -1.576807]\n","\tCycle Losses: [x: 0.032966] [z_n: 0.027394] [z_c: 0.000206]\n","[Epoch 49/200] \n","\tModel Losses: [D: -0.703355] [GE: -1.566419]\n","\tCycle Losses: [x: 0.035568] [z_n: 0.025591] [z_c: 0.000135]\n","[Epoch 50/200] \n","\tModel Losses: [D: -0.709169] [GE: -1.791158]\n","\tCycle Losses: [x: 0.033627] [z_n: 0.026075] [z_c: 0.000069]\n","[Epoch 51/200] \n","\tModel Losses: [D: -0.585493] [GE: -0.898774]\n","\tCycle Losses: [x: 0.035531] [z_n: 0.026788] [z_c: 0.000056]\n","[Epoch 52/200] \n","\tModel Losses: [D: -0.734479] [GE: -1.275047]\n","\tCycle Losses: [x: 0.036192] [z_n: 0.024682] [z_c: 0.000302]\n","[Epoch 53/200] \n","\tModel Losses: [D: -0.760712] [GE: -1.678262]\n","\tCycle Losses: [x: 0.036563] [z_n: 0.024065] [z_c: 0.000024]\n","[Epoch 54/200] \n","\tModel Losses: [D: -0.741795] [GE: -1.424983]\n","\tCycle Losses: [x: 0.036268] [z_n: 0.023558] [z_c: 0.000044]\n","[Epoch 55/200] \n","\tModel Losses: [D: -0.498604] [GE: -1.086916]\n","\tCycle Losses: [x: 0.037148] [z_n: 0.028646] [z_c: 0.000102]\n","[Epoch 56/200] \n","\tModel Losses: [D: -0.567796] [GE: -0.368636]\n","\tCycle Losses: [x: 0.037369] [z_n: 0.026488] [z_c: 0.008164]\n","[Epoch 57/200] \n","\tModel Losses: [D: -0.600983] [GE: -1.143150]\n","\tCycle Losses: [x: 0.037768] [z_n: 0.025925] [z_c: 0.000026]\n","[Epoch 58/200] \n","\tModel Losses: [D: -0.573431] [GE: -1.283406]\n","\tCycle Losses: [x: 0.037414] [z_n: 0.027487] [z_c: 0.001271]\n","[Epoch 59/200] \n","\tModel Losses: [D: -0.649203] [GE: -1.060241]\n","\tCycle Losses: [x: 0.037490] [z_n: 0.023419] [z_c: 0.000059]\n","[Epoch 60/200] \n","\tModel Losses: [D: -0.621703] [GE: -1.295846]\n","\tCycle Losses: [x: 0.038668] [z_n: 0.022902] [z_c: 0.000092]\n","[Epoch 61/200] \n","\tModel Losses: [D: -0.495760] [GE: -1.875537]\n","\tCycle Losses: [x: 0.039708] [z_n: 0.023194] [z_c: 0.000245]\n","[Epoch 62/200] \n","\tModel Losses: [D: -0.692253] [GE: -1.020190]\n","\tCycle Losses: [x: 0.038887] [z_n: 0.023126] [z_c: 0.000119]\n","[Epoch 63/200] \n","\tModel Losses: [D: -0.667188] [GE: -1.690053]\n","\tCycle Losses: [x: 0.040063] [z_n: 0.020808] [z_c: 0.000834]\n","[Epoch 64/200] \n","\tModel Losses: [D: -0.670078] [GE: -1.728096]\n","\tCycle Losses: [x: 0.040051] [z_n: 0.023586] [z_c: 0.000962]\n","[Epoch 65/200] \n","\tModel Losses: [D: -0.778177] [GE: -2.184428]\n","\tCycle Losses: [x: 0.040384] [z_n: 0.024066] [z_c: 0.000531]\n","[Epoch 66/200] \n","\tModel Losses: [D: -0.743828] [GE: -2.049867]\n","\tCycle Losses: [x: 0.041861] [z_n: 0.021143] [z_c: 0.000081]\n","[Epoch 67/200] \n","\tModel Losses: [D: -0.417541] [GE: -2.067544]\n","\tCycle Losses: [x: 0.040238] [z_n: 0.021616] [z_c: 0.000072]\n","[Epoch 68/200] \n","\tModel Losses: [D: -0.785691] [GE: -2.607816]\n","\tCycle Losses: [x: 0.040840] [z_n: 0.021673] [z_c: 0.000083]\n","[Epoch 69/200] \n","\tModel Losses: [D: -0.757295] [GE: -1.577322]\n","\tCycle Losses: [x: 0.041651] [z_n: 0.023551] [z_c: 0.000156]\n","[Epoch 70/200] \n","\tModel Losses: [D: -0.673139] [GE: -1.443017]\n","\tCycle Losses: [x: 0.044495] [z_n: 0.023997] [z_c: 0.000034]\n","[Epoch 71/200] \n","\tModel Losses: [D: -0.760121] [GE: -2.358607]\n","\tCycle Losses: [x: 0.044755] [z_n: 0.019022] [z_c: 0.000053]\n","[Epoch 72/200] \n","\tModel Losses: [D: -0.863021] [GE: -2.269957]\n","\tCycle Losses: [x: 0.042322] [z_n: 0.023329] [z_c: 0.002125]\n","[Epoch 73/200] \n","\tModel Losses: [D: -0.664461] [GE: -1.266959]\n","\tCycle Losses: [x: 0.040575] [z_n: 0.023002] [z_c: 0.000317]\n","[Epoch 74/200] \n","\tModel Losses: [D: -0.585130] [GE: -1.992104]\n","\tCycle Losses: [x: 0.043893] [z_n: 0.017750] [z_c: 0.000011]\n","[Epoch 75/200] \n","\tModel Losses: [D: -0.637816] [GE: -1.804673]\n","\tCycle Losses: [x: 0.042680] [z_n: 0.021071] [z_c: 0.000075]\n","[Epoch 76/200] \n","\tModel Losses: [D: -0.640507] [GE: -2.315850]\n","\tCycle Losses: [x: 0.042694] [z_n: 0.020283] [z_c: 0.000092]\n","[Epoch 77/200] \n","\tModel Losses: [D: -0.530237] [GE: -1.944158]\n","\tCycle Losses: [x: 0.040840] [z_n: 0.022119] [z_c: 0.003672]\n","[Epoch 78/200] \n","\tModel Losses: [D: -0.705274] [GE: -1.286564]\n","\tCycle Losses: [x: 0.042779] [z_n: 0.021865] [z_c: 0.000044]\n","[Epoch 79/200] \n","\tModel Losses: [D: -0.707749] [GE: -2.476117]\n","\tCycle Losses: [x: 0.042983] [z_n: 0.022596] [z_c: 0.000751]\n","[Epoch 80/200] \n","\tModel Losses: [D: -0.541755] [GE: -1.942390]\n","\tCycle Losses: [x: 0.044782] [z_n: 0.021144] [z_c: 0.000567]\n","[Epoch 81/200] \n","\tModel Losses: [D: -0.648400] [GE: -1.848873]\n","\tCycle Losses: [x: 0.045363] [z_n: 0.020065] [z_c: 0.000101]\n","[Epoch 82/200] \n","\tModel Losses: [D: -0.709584] [GE: -2.584013]\n","\tCycle Losses: [x: 0.043823] [z_n: 0.020449] [z_c: 0.000123]\n","[Epoch 83/200] \n","\tModel Losses: [D: -0.424561] [GE: -1.446006]\n","\tCycle Losses: [x: 0.046624] [z_n: 0.019184] [z_c: 0.000074]\n","[Epoch 84/200] \n","\tModel Losses: [D: -0.592149] [GE: -2.599085]\n","\tCycle Losses: [x: 0.045251] [z_n: 0.017488] [z_c: 0.000079]\n","[Epoch 85/200] \n","\tModel Losses: [D: -0.872236] [GE: -1.306296]\n","\tCycle Losses: [x: 0.047009] [z_n: 0.020468] [z_c: 0.001018]\n","[Epoch 86/200] \n","\tModel Losses: [D: -0.597057] [GE: -2.281789]\n","\tCycle Losses: [x: 0.046544] [z_n: 0.022471] [z_c: 0.000111]\n","[Epoch 87/200] \n","\tModel Losses: [D: -0.515574] [GE: -2.129538]\n","\tCycle Losses: [x: 0.045862] [z_n: 0.020009] [z_c: 0.000047]\n","[Epoch 88/200] \n","\tModel Losses: [D: -0.516054] [GE: -2.985722]\n","\tCycle Losses: [x: 0.045362] [z_n: 0.021654] [z_c: 0.000271]\n","[Epoch 89/200] \n","\tModel Losses: [D: -0.754518] [GE: -2.100727]\n","\tCycle Losses: [x: 0.046121] [z_n: 0.021803] [z_c: 0.000050]\n","[Epoch 90/200] \n","\tModel Losses: [D: -0.681421] [GE: -1.565211]\n","\tCycle Losses: [x: 0.048326] [z_n: 0.019357] [z_c: 0.000114]\n","[Epoch 91/200] \n","\tModel Losses: [D: -0.562359] [GE: -1.463453]\n","\tCycle Losses: [x: 0.047760] [z_n: 0.018030] [z_c: 0.000281]\n","[Epoch 92/200] \n","\tModel Losses: [D: -0.655262] [GE: -1.458343]\n","\tCycle Losses: [x: 0.048982] [z_n: 0.015580] [z_c: 0.000884]\n","[Epoch 93/200] \n","\tModel Losses: [D: -0.567048] [GE: -2.143464]\n","\tCycle Losses: [x: 0.049309] [z_n: 0.020248] [z_c: 0.000067]\n","[Epoch 94/200] \n","\tModel Losses: [D: -0.703361] [GE: -1.965296]\n","\tCycle Losses: [x: 0.046995] [z_n: 0.021875] [z_c: 0.000034]\n","[Epoch 95/200] \n","\tModel Losses: [D: -0.688027] [GE: -1.753798]\n","\tCycle Losses: [x: 0.048203] [z_n: 0.015095] [z_c: 0.000050]\n","[Epoch 96/200] \n","\tModel Losses: [D: -0.743457] [GE: -2.064836]\n","\tCycle Losses: [x: 0.048670] [z_n: 0.018832] [z_c: 0.000060]\n","[Epoch 97/200] \n","\tModel Losses: [D: -0.552044] [GE: -2.220438]\n","\tCycle Losses: [x: 0.049672] [z_n: 0.018937] [z_c: 0.000115]\n","[Epoch 98/200] \n","\tModel Losses: [D: -0.505539] [GE: -2.245197]\n","\tCycle Losses: [x: 0.050909] [z_n: 0.018981] [z_c: 0.007650]\n","[Epoch 99/200] \n","\tModel Losses: [D: -0.705680] [GE: -2.161238]\n","\tCycle Losses: [x: 0.048786] [z_n: 0.020533] [z_c: 0.000237]\n","[Epoch 100/200] \n","\tModel Losses: [D: -0.492637] [GE: -1.478544]\n","\tCycle Losses: [x: 0.046057] [z_n: 0.017736] [z_c: 0.000123]\n","[Epoch 101/200] \n","\tModel Losses: [D: -0.583076] [GE: -2.080768]\n","\tCycle Losses: [x: 0.048815] [z_n: 0.015065] [z_c: 0.000042]\n","[Epoch 102/200] \n","\tModel Losses: [D: -0.560788] [GE: -1.448387]\n","\tCycle Losses: [x: 0.051207] [z_n: 0.017258] [z_c: 0.000095]\n","[Epoch 103/200] \n","\tModel Losses: [D: -0.715891] [GE: -1.937637]\n","\tCycle Losses: [x: 0.047073] [z_n: 0.018926] [z_c: 0.000066]\n","[Epoch 104/200] \n","\tModel Losses: [D: -0.681326] [GE: -2.167593]\n","\tCycle Losses: [x: 0.049226] [z_n: 0.017419] [z_c: 0.000143]\n","[Epoch 105/200] \n","\tModel Losses: [D: -0.572687] [GE: -1.704222]\n","\tCycle Losses: [x: 0.049037] [z_n: 0.016226] [z_c: 0.000125]\n","[Epoch 106/200] \n","\tModel Losses: [D: -0.635161] [GE: -1.940679]\n","\tCycle Losses: [x: 0.046028] [z_n: 0.018486] [z_c: 0.000104]\n","[Epoch 107/200] \n","\tModel Losses: [D: -0.581072] [GE: -2.321834]\n","\tCycle Losses: [x: 0.048955] [z_n: 0.017470] [z_c: 0.000051]\n","[Epoch 108/200] \n","\tModel Losses: [D: -0.760362] [GE: -1.180430]\n","\tCycle Losses: [x: 0.047232] [z_n: 0.018804] [z_c: 0.000527]\n","[Epoch 109/200] \n","\tModel Losses: [D: -0.569636] [GE: -1.515572]\n","\tCycle Losses: [x: 0.047311] [z_n: 0.017094] [z_c: 0.000754]\n","[Epoch 110/200] \n","\tModel Losses: [D: -0.620071] [GE: -1.555474]\n","\tCycle Losses: [x: 0.048416] [z_n: 0.018210] [z_c: 0.000031]\n","[Epoch 111/200] \n","\tModel Losses: [D: -0.474115] [GE: -1.759407]\n","\tCycle Losses: [x: 0.047258] [z_n: 0.016272] [z_c: 0.000402]\n","[Epoch 112/200] \n","\tModel Losses: [D: -0.563118] [GE: -2.031466]\n","\tCycle Losses: [x: 0.047072] [z_n: 0.014721] [z_c: 0.000029]\n","[Epoch 113/200] \n","\tModel Losses: [D: -0.627490] [GE: -1.421334]\n","\tCycle Losses: [x: 0.048424] [z_n: 0.016348] [z_c: 0.000137]\n","[Epoch 114/200] \n","\tModel Losses: [D: -0.470077] [GE: -2.390856]\n","\tCycle Losses: [x: 0.048122] [z_n: 0.015064] [z_c: 0.000082]\n","[Epoch 115/200] \n","\tModel Losses: [D: -0.540939] [GE: -2.083035]\n","\tCycle Losses: [x: 0.051459] [z_n: 0.016499] [z_c: 0.000045]\n","[Epoch 116/200] \n","\tModel Losses: [D: -0.452141] [GE: -1.170414]\n","\tCycle Losses: [x: 0.048964] [z_n: 0.014629] [z_c: 0.000012]\n","[Epoch 117/200] \n","\tModel Losses: [D: -0.843862] [GE: -2.159616]\n","\tCycle Losses: [x: 0.049691] [z_n: 0.014122] [z_c: 0.000482]\n","[Epoch 118/200] \n","\tModel Losses: [D: -0.577868] [GE: -2.309804]\n","\tCycle Losses: [x: 0.050451] [z_n: 0.018856] [z_c: 0.000116]\n","[Epoch 119/200] \n","\tModel Losses: [D: -0.627048] [GE: -1.418561]\n","\tCycle Losses: [x: 0.051201] [z_n: 0.014050] [z_c: 0.000198]\n","[Epoch 120/200] \n","\tModel Losses: [D: -0.405149] [GE: -1.838642]\n","\tCycle Losses: [x: 0.051159] [z_n: 0.016678] [z_c: 0.000398]\n","[Epoch 121/200] \n","\tModel Losses: [D: -0.566069] [GE: -1.678405]\n","\tCycle Losses: [x: 0.050854] [z_n: 0.014166] [z_c: 0.000737]\n","[Epoch 122/200] \n","\tModel Losses: [D: -0.514295] [GE: -2.340743]\n","\tCycle Losses: [x: 0.049259] [z_n: 0.014003] [z_c: 0.000060]\n","[Epoch 123/200] \n","\tModel Losses: [D: -0.473322] [GE: -1.046485]\n","\tCycle Losses: [x: 0.051507] [z_n: 0.014240] [z_c: 0.000568]\n","[Epoch 124/200] \n","\tModel Losses: [D: -0.526518] [GE: -1.464829]\n","\tCycle Losses: [x: 0.050396] [z_n: 0.016026] [z_c: 0.000029]\n","[Epoch 125/200] \n","\tModel Losses: [D: -0.649438] [GE: -1.705589]\n","\tCycle Losses: [x: 0.050820] [z_n: 0.020752] [z_c: 0.005120]\n","[Epoch 126/200] \n","\tModel Losses: [D: -0.676979] [GE: -1.724041]\n","\tCycle Losses: [x: 0.052333] [z_n: 0.014462] [z_c: 0.000052]\n","[Epoch 127/200] \n","\tModel Losses: [D: -0.429107] [GE: -1.355995]\n","\tCycle Losses: [x: 0.050960] [z_n: 0.014774] [z_c: 0.000078]\n","[Epoch 128/200] \n","\tModel Losses: [D: -0.550821] [GE: -1.818843]\n","\tCycle Losses: [x: 0.049970] [z_n: 0.014320] [z_c: 0.000123]\n","[Epoch 129/200] \n","\tModel Losses: [D: -0.489071] [GE: -1.525903]\n","\tCycle Losses: [x: 0.049030] [z_n: 0.014993] [z_c: 0.003873]\n","[Epoch 130/200] \n","\tModel Losses: [D: -0.623203] [GE: -1.250224]\n","\tCycle Losses: [x: 0.049695] [z_n: 0.016186] [z_c: 0.000269]\n","[Epoch 131/200] \n","\tModel Losses: [D: -0.448421] [GE: -1.591399]\n","\tCycle Losses: [x: 0.048281] [z_n: 0.017151] [z_c: 0.000190]\n","[Epoch 132/200] \n","\tModel Losses: [D: -0.629330] [GE: -1.570381]\n","\tCycle Losses: [x: 0.048741] [z_n: 0.015479] [z_c: 0.000176]\n","[Epoch 133/200] \n","\tModel Losses: [D: -0.489125] [GE: -1.913888]\n","\tCycle Losses: [x: 0.049722] [z_n: 0.013637] [z_c: 0.000241]\n","[Epoch 134/200] \n","\tModel Losses: [D: -0.557437] [GE: -1.901873]\n","\tCycle Losses: [x: 0.051340] [z_n: 0.014842] [z_c: 0.000031]\n","[Epoch 135/200] \n","\tModel Losses: [D: -0.607210] [GE: -1.881780]\n","\tCycle Losses: [x: 0.049608] [z_n: 0.015192] [z_c: 0.000084]\n","[Epoch 136/200] \n","\tModel Losses: [D: -0.550541] [GE: -2.315248]\n","\tCycle Losses: [x: 0.049231] [z_n: 0.014990] [z_c: 0.000070]\n","[Epoch 137/200] \n","\tModel Losses: [D: -0.453008] [GE: -2.300388]\n","\tCycle Losses: [x: 0.050901] [z_n: 0.013602] [z_c: 0.000322]\n","[Epoch 138/200] \n","\tModel Losses: [D: -0.589645] [GE: -2.325890]\n","\tCycle Losses: [x: 0.051985] [z_n: 0.015217] [z_c: 0.000147]\n","[Epoch 139/200] \n","\tModel Losses: [D: -0.744895] [GE: -1.698895]\n","\tCycle Losses: [x: 0.051534] [z_n: 0.012414] [z_c: 0.000151]\n","[Epoch 140/200] \n","\tModel Losses: [D: -0.459035] [GE: -1.647777]\n","\tCycle Losses: [x: 0.050603] [z_n: 0.013988] [z_c: 0.000136]\n","[Epoch 141/200] \n","\tModel Losses: [D: -0.628247] [GE: -1.681400]\n","\tCycle Losses: [x: 0.051378] [z_n: 0.015487] [z_c: 0.000061]\n","[Epoch 142/200] \n","\tModel Losses: [D: -0.624250] [GE: -2.743657]\n","\tCycle Losses: [x: 0.052068] [z_n: 0.013920] [z_c: 0.000332]\n","[Epoch 143/200] \n","\tModel Losses: [D: -0.627042] [GE: -1.845596]\n","\tCycle Losses: [x: 0.050618] [z_n: 0.015166] [z_c: 0.000053]\n","[Epoch 144/200] \n","\tModel Losses: [D: -0.548805] [GE: -1.813090]\n","\tCycle Losses: [x: 0.050203] [z_n: 0.013953] [z_c: 0.000051]\n","[Epoch 145/200] \n","\tModel Losses: [D: -0.560678] [GE: -2.372517]\n","\tCycle Losses: [x: 0.051376] [z_n: 0.015846] [z_c: 0.000360]\n","[Epoch 146/200] \n","\tModel Losses: [D: -0.600861] [GE: -1.957652]\n","\tCycle Losses: [x: 0.050051] [z_n: 0.016201] [z_c: 0.000070]\n","[Epoch 147/200] \n","\tModel Losses: [D: -0.353876] [GE: -1.741278]\n","\tCycle Losses: [x: 0.052135] [z_n: 0.013970] [z_c: 0.000137]\n","[Epoch 148/200] \n","\tModel Losses: [D: -0.513493] [GE: -1.945568]\n","\tCycle Losses: [x: 0.051748] [z_n: 0.014153] [z_c: 0.000163]\n","[Epoch 149/200] \n","\tModel Losses: [D: -0.358299] [GE: -2.639390]\n","\tCycle Losses: [x: 0.053552] [z_n: 0.014488] [z_c: 0.000665]\n","[Epoch 150/200] \n","\tModel Losses: [D: -0.426313] [GE: -3.109417]\n","\tCycle Losses: [x: 0.050914] [z_n: 0.012044] [z_c: 0.000062]\n","[Epoch 151/200] \n","\tModel Losses: [D: -0.432479] [GE: -1.936789]\n","\tCycle Losses: [x: 0.052789] [z_n: 0.014459] [z_c: 0.000029]\n","[Epoch 152/200] \n","\tModel Losses: [D: -0.620727] [GE: -2.122268]\n","\tCycle Losses: [x: 0.051497] [z_n: 0.012888] [z_c: 0.000188]\n","[Epoch 153/200] \n","\tModel Losses: [D: -0.505744] [GE: -2.169339]\n","\tCycle Losses: [x: 0.052816] [z_n: 0.013479] [z_c: 0.000053]\n","[Epoch 154/200] \n","\tModel Losses: [D: -0.520377] [GE: -2.194663]\n","\tCycle Losses: [x: 0.051352] [z_n: 0.012685] [z_c: 0.000066]\n","[Epoch 155/200] \n","\tModel Losses: [D: -0.506596] [GE: -2.335184]\n","\tCycle Losses: [x: 0.051617] [z_n: 0.012230] [z_c: 0.001067]\n","[Epoch 156/200] \n","\tModel Losses: [D: -0.512140] [GE: -2.611591]\n","\tCycle Losses: [x: 0.048550] [z_n: 0.015485] [z_c: 0.000097]\n","[Epoch 157/200] \n","\tModel Losses: [D: -0.382227] [GE: -2.390612]\n","\tCycle Losses: [x: 0.053697] [z_n: 0.013186] [z_c: 0.000102]\n","[Epoch 158/200] \n","\tModel Losses: [D: -0.370500] [GE: -2.250177]\n","\tCycle Losses: [x: 0.050942] [z_n: 0.014252] [z_c: 0.000104]\n","[Epoch 159/200] \n","\tModel Losses: [D: -0.286528] [GE: -2.186797]\n","\tCycle Losses: [x: 0.051219] [z_n: 0.012002] [z_c: 0.000093]\n","[Epoch 160/200] \n","\tModel Losses: [D: -0.303690] [GE: -2.168741]\n","\tCycle Losses: [x: 0.051452] [z_n: 0.012040] [z_c: 0.000329]\n","[Epoch 161/200] \n","\tModel Losses: [D: -0.585616] [GE: -2.988747]\n","\tCycle Losses: [x: 0.051427] [z_n: 0.012854] [z_c: 0.000092]\n","[Epoch 162/200] \n","\tModel Losses: [D: -0.463446] [GE: -2.003642]\n","\tCycle Losses: [x: 0.046247] [z_n: 0.011655] [z_c: 0.000093]\n","[Epoch 163/200] \n","\tModel Losses: [D: -0.644700] [GE: -1.774555]\n","\tCycle Losses: [x: 0.049210] [z_n: 0.012448] [z_c: 0.000089]\n","[Epoch 164/200] \n","\tModel Losses: [D: -0.453876] [GE: -2.397764]\n","\tCycle Losses: [x: 0.051363] [z_n: 0.011015] [z_c: 0.000026]\n","[Epoch 165/200] \n","\tModel Losses: [D: -0.595897] [GE: -1.772114]\n","\tCycle Losses: [x: 0.051581] [z_n: 0.014315] [z_c: 0.000114]\n","[Epoch 166/200] \n","\tModel Losses: [D: -0.574167] [GE: -2.555458]\n","\tCycle Losses: [x: 0.049763] [z_n: 0.013235] [z_c: 0.000044]\n","[Epoch 167/200] \n","\tModel Losses: [D: -0.505834] [GE: -1.748751]\n","\tCycle Losses: [x: 0.052257] [z_n: 0.009736] [z_c: 0.000721]\n","[Epoch 168/200] \n","\tModel Losses: [D: -0.564734] [GE: -2.400483]\n","\tCycle Losses: [x: 0.049827] [z_n: 0.011742] [z_c: 0.000080]\n","[Epoch 169/200] \n","\tModel Losses: [D: -0.461478] [GE: -2.021176]\n","\tCycle Losses: [x: 0.051137] [z_n: 0.013277] [z_c: 0.000076]\n","[Epoch 170/200] \n","\tModel Losses: [D: -0.473770] [GE: -2.364890]\n","\tCycle Losses: [x: 0.049670] [z_n: 0.014287] [z_c: 0.000026]\n","[Epoch 171/200] \n","\tModel Losses: [D: -0.551337] [GE: -2.059244]\n","\tCycle Losses: [x: 0.050072] [z_n: 0.011647] [z_c: 0.000037]\n","[Epoch 172/200] \n","\tModel Losses: [D: -0.406963] [GE: -2.179205]\n","\tCycle Losses: [x: 0.049767] [z_n: 0.014002] [z_c: 0.000095]\n","[Epoch 173/200] \n","\tModel Losses: [D: -0.443949] [GE: -2.407969]\n","\tCycle Losses: [x: 0.051586] [z_n: 0.013472] [z_c: 0.000171]\n","[Epoch 174/200] \n","\tModel Losses: [D: -0.637444] [GE: -2.074823]\n","\tCycle Losses: [x: 0.049785] [z_n: 0.015345] [z_c: 0.000013]\n","[Epoch 175/200] \n","\tModel Losses: [D: -0.586793] [GE: -2.299439]\n","\tCycle Losses: [x: 0.050322] [z_n: 0.014420] [z_c: 0.000236]\n","[Epoch 176/200] \n","\tModel Losses: [D: -0.426334] [GE: -2.301505]\n","\tCycle Losses: [x: 0.051702] [z_n: 0.016787] [z_c: 0.000260]\n","[Epoch 177/200] \n","\tModel Losses: [D: -0.459294] [GE: -2.943791]\n","\tCycle Losses: [x: 0.049619] [z_n: 0.009659] [z_c: 0.000049]\n","[Epoch 178/200] \n","\tModel Losses: [D: -0.612704] [GE: -2.584564]\n","\tCycle Losses: [x: 0.052517] [z_n: 0.012570] [z_c: 0.000215]\n","[Epoch 179/200] \n","\tModel Losses: [D: -0.057711] [GE: -2.618072]\n","\tCycle Losses: [x: 0.049458] [z_n: 0.013614] [z_c: 0.000040]\n","[Epoch 180/200] \n","\tModel Losses: [D: -0.364443] [GE: -2.045184]\n","\tCycle Losses: [x: 0.051192] [z_n: 0.014228] [z_c: 0.000315]\n","[Epoch 181/200] \n","\tModel Losses: [D: -0.496490] [GE: -2.462887]\n","\tCycle Losses: [x: 0.050906] [z_n: 0.009462] [z_c: 0.000060]\n","[Epoch 182/200] \n","\tModel Losses: [D: -0.458481] [GE: -1.830657]\n","\tCycle Losses: [x: 0.050846] [z_n: 0.010718] [z_c: 0.000081]\n","[Epoch 183/200] \n","\tModel Losses: [D: -0.566215] [GE: -2.843471]\n","\tCycle Losses: [x: 0.051678] [z_n: 0.013258] [z_c: 0.000014]\n","[Epoch 184/200] \n","\tModel Losses: [D: -0.550880] [GE: -2.712953]\n","\tCycle Losses: [x: 0.052162] [z_n: 0.010965] [z_c: 0.000131]\n","[Epoch 185/200] \n","\tModel Losses: [D: -0.523415] [GE: -2.932867]\n","\tCycle Losses: [x: 0.051042] [z_n: 0.012784] [z_c: 0.000037]\n","[Epoch 186/200] \n","\tModel Losses: [D: -0.410798] [GE: -2.651129]\n","\tCycle Losses: [x: 0.050396] [z_n: 0.015178] [z_c: 0.000040]\n","[Epoch 187/200] \n","\tModel Losses: [D: -0.479130] [GE: -2.916785]\n","\tCycle Losses: [x: 0.051709] [z_n: 0.013426] [z_c: 0.000046]\n","[Epoch 188/200] \n","\tModel Losses: [D: -0.604114] [GE: -2.814271]\n","\tCycle Losses: [x: 0.050315] [z_n: 0.012182] [z_c: 0.000066]\n","[Epoch 189/200] \n","\tModel Losses: [D: -0.604360] [GE: -2.831169]\n","\tCycle Losses: [x: 0.050238] [z_n: 0.010012] [z_c: 0.000156]\n","[Epoch 190/200] \n","\tModel Losses: [D: -0.326460] [GE: -3.152242]\n","\tCycle Losses: [x: 0.051901] [z_n: 0.013006] [z_c: 0.000160]\n","[Epoch 191/200] \n","\tModel Losses: [D: -0.372482] [GE: -2.759083]\n","\tCycle Losses: [x: 0.049166] [z_n: 0.011954] [z_c: 0.000064]\n","[Epoch 192/200] \n","\tModel Losses: [D: -0.507727] [GE: -2.728605]\n","\tCycle Losses: [x: 0.051703] [z_n: 0.012981] [z_c: 0.000347]\n","[Epoch 193/200] \n","\tModel Losses: [D: -0.487734] [GE: -2.315573]\n","\tCycle Losses: [x: 0.050714] [z_n: 0.013316] [z_c: 0.000640]\n","[Epoch 194/200] \n","\tModel Losses: [D: -0.405848] [GE: -2.507076]\n","\tCycle Losses: [x: 0.051732] [z_n: 0.013904] [z_c: 0.000018]\n","[Epoch 195/200] \n","\tModel Losses: [D: -0.595678] [GE: -2.282040]\n","\tCycle Losses: [x: 0.054846] [z_n: 0.010520] [z_c: 0.000034]\n","[Epoch 196/200] \n","\tModel Losses: [D: -0.199795] [GE: -2.145168]\n","\tCycle Losses: [x: 0.050011] [z_n: 0.011665] [z_c: 0.000139]\n","[Epoch 197/200] \n","\tModel Losses: [D: -0.268100] [GE: -2.567068]\n","\tCycle Losses: [x: 0.053478] [z_n: 0.010247] [z_c: 0.000078]\n","[Epoch 198/200] \n","\tModel Losses: [D: -0.591770] [GE: -2.804492]\n","\tCycle Losses: [x: 0.052465] [z_n: 0.011765] [z_c: 0.000070]\n","[Epoch 199/200] \n","\tModel Losses: [D: -0.428163] [GE: -3.086559]\n","\tCycle Losses: [x: 0.052465] [z_n: 0.011362] [z_c: 0.000048]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZuqCHV8v_dpX"},"execution_count":null,"outputs":[]}]}