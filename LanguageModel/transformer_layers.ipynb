{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOzdS1tghJ/yOsmrOSjLFYn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"F2T4XquxeL4L"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","source":["### Token Embedding"],"metadata":{"id":"qWrKzY375Srf"}},{"cell_type":"code","source":["# Token Embedding\n","\n","class Token_Embedding(nn.Embedding):\n","  def __init__(self, vocab_size, d_model):\n","    super(Token_Embedding, self).__init__(vocab_size, d_model, padding_idx = 1)"],"metadata":{"id":"hgTiX7Gz5SJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Positional Encoding"],"metadata":{"id":"tLAQWIRrBNP2"}},{"cell_type":"code","source":["# Positional Encoding\n","\n","class Positional_Encoding(nn.Module):\n","  def __init__(self, max_len, d_model):\n","    super(Positional_Encoding, self).__init__()\n","    self.pos_encoding = self.positional_encoding(max_len, d_model)\n","\n","  def get_angle(self, pos, i, d_model):\n","    angle = 1/np.power(10000,2*(i//2)/d_model)\n","    return pos * angle\n","\n","  def positional_encoding(self, max_len, d_model):\n","    pos = self.get_angle(pos = torch.arange(0, max_len, dtype=torch.float32).view(-1,1),\n","                         i = torch.arange(0, d_model, dtype=torch.float32).view(1,-1),\n","                         d_model = d_model)\n","    pos[:,0::2] = torch.sin(pos[:,0::2])\n","    pos[:,1::2] = torch.cos(pos[:,1::2])\n","    return pos\n","  \n","  def forward(self, input):\n","    batch_size, seq_len, d_model = input.size()\n","    return self.pos_encoding[:seq_len,:]\n"],"metadata":{"id":"1ZQikg-key0P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Embedding Layer"],"metadata":{"id":"dKGqkU_F50I8"}},{"cell_type":"code","source":["# Transformer Embedding\n","\n","class Transformer_Embedding(nn.Module):\n","  def __init__(self, vocab_size, max_len, d_model, drop_rate):\n","    super(Transformer_Embedding, self).__init__()\n","    self.tok_embedding = Token_Embedding(vocab_size, d_model)\n","    self.pos_embedding = Positional_Encoding(max_len, d_model)\n","    self.dropout = nn.Dropout(drop_rate)\n","\n","  def forward(self, x):\n","    tok_emb = self.tok_embedding(x)\n","    pos_emb = self.pos_embedding(x)\n","    return self.drop_out(tok_emb + pos_emb) "],"metadata":{"id":"M3vOh6gU53wt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multi-head Attention"],"metadata":{"id":"icPfjg7HBQ5M"}},{"cell_type":"code","source":["# Self_attention\n","\n","class Self_Attention(nn.Module):\n","  def __init__(self):\n","    super(Self_Attention, self).__init__()\n","    # dim=-1 : last dimension, dim 방향으로 sum 했을때 1로 기억하자\n","    self.softmax = nn.Softmax(dim=-1)\n","\n","  def forward(self, Q, K, V, mask=None):\n","    # input : (batch_size, n_head, seq_len, d_k)\n","    # output : (batch_size, n_head, seq_len, d_k)\n","    batch_size, head, seq_len, d_k = K.size()\n","    # dot product\n","    K_t = K.transpose(2,3)\n","    score = Q @ K_t / np.sqrt(d_k)\n","    # masking\n","    if mask is not None:\n","      score = score.masked_fill(mask==0, -10000)\n","    # softmax\n","    score = self.softmax(score)\n","    # multiply V\n","    V  = score @ V\n","    return V, score\n","\n","class MultiHead_Attention(nn.Module):\n","  def __init__(self, d_model, n_head):\n","    super(MultiHead_Attention, self).__init__()\n","    self.n_head = n_head\n","    self.attention = Self_Attention()\n","    self.W_Q = nn.Linear(d_model, d_model)\n","    self.W_K = nn.Linear(d_model, d_model)\n","    self.W_V = nn.Linear(d_model, d_model)\n","    self.W_concat = nn.Linear(d_model, d_model)\n","  \n","  def split(self, tensor):\n","    # input : (batch_size, seq_len, d_model)\n","    # output : (batch_size, n_head, seq_len, d_k)\n","    batch_size, seq_len, d_model = tensor.size()\n","\n","    d_k = d_model//self.n_head\n","    tensor = tensor.view(batch_size, seq_len, self.n_head, d_k).transpose(1,2)\n","    return tensor\n","\n","  def concat(self, tensor):\n","    # input : (batch_size, n_head, seq_len, d_k)\n","    # output : (batch_size, seq_len, d_model)\n","    batch_size, n_head, seq_len, d_k = tensor.size()\n","\n","    d_model = n_head * d_k\n","    # transpose, view로만 변환시킬 경우 기존텐서와 메모리를 공유하고 모양만 변함\n","    # 주소값 연속성이 불변인 것이 문제, contiguous로 새로운 공간에 데이터 복사\n","    tensor = tensor.transpose(1,2).contiguous().view(batch_size, seq_len, d_model)\n","    return tensor\n","\n","  def forward(self, Q, K, V, mask=None):\n","    # dot product\n","    Q,K,V = self.W_Q(Q), self.W_K(K), self.W_V(V)\n","    # split tensor by n_head\n","    Q,K,V = self.split(Q),self.split(K),self.split(V)\n","    # scale dot product attention\n","    out, attention = self.attention(Q,K,V,mask=mask)\n","    # out : (batch_size, n_head, seq_len, d_k), attention : (batch_size, n_head, seq_len, seq_len)\n","    out = self.concat(out)\n","    out = self.W_concat(out)\n","    return out\n"],"metadata":{"id":"jIt4Cq3Xgym8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Layer Normalization"],"metadata":{"id":"D73uZqiPBWdA"}},{"cell_type":"code","source":["# Layer Norm\n","class Layer_Normalization(nn.Module):\n","  def __init__(self, d_model, eps = 1e-12):\n","    super(Layer_Normalization, self).__init__()\n","    self.gamma = nn.Parameter(torch.ones(d_model))\n","    self.beta = nn.Parameter(torch.zeros(d_model))\n","    self.eps = eps\n","\n","  def forward(self, x):\n","    mean = x.mean(dim=-1, keepdim=True)\n","    var = x.var(dim=-1, keepdim=True, unbiased=False)\n","\n","    out = (x-mean) / torch.sqrt(var + self.eps)\n","    out = self.gamma * out + self.beta\n","    return out"],"metadata":{"id":"bUe-kEYZqJSP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Feed-Forward Network"],"metadata":{"id":"39lLZMp80u0B"}},{"cell_type":"code","source":["# FFN\n","\n","class Feed_Forward_Network(nn.Module):\n","  def __init__(self, d_model, hidden, drop_rate = 0.1):\n","    super(Feed_Forward_Network, self).__init__()\n","    self.linear1 = nn.Linear(d_model, hidden)\n","    self.linear2 = nn.Linear(hidden, d_model)\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.Dropout(p = drop_rate)\n","\n","  def forward(self, x):\n","    out = self.linear1(x)\n","    out = self.relu(out)\n","    out = self.dropout(out)\n","    out = self.linear2(out)\n","    return out"],"metadata":{"id":"YVvfoxbUoTB4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Encoder Layer"],"metadata":{"id":"LTC_UX7T2MP1"}},{"cell_type":"code","source":["# Encoder Layer\n","\n","class Encoder_Layer(nn.Module):\n","  def __init__(self, d_model, ffn_hidden, n_head, drop_rate):\n","    super(Encoder_Layer, self).__init__()\n","    self.attention = MultiHead_Attention(d_model, n_head)\n","    self.layer_norm1 = Layer_Normalization(d_model)\n","    self.dropout1 = nn.Dropout(drop_rate)\n","    self.ffn = Feed_Forward_Network(d_model, ffn_hidden, drop_rate)\n","    self.layer_norm2 = Layer_Normalization(d_model)\n","    self.dropout2 = nn.Dropout(drop_rate)\n","\n","  def forward(self, x, src_mask):\n","    x0 = x\n","    x = self.attention(x,x,x,src_mask)\n","    x = self.dropout1(x)\n","    x = self.layer_norm1(x+x0)\n","    \n","    x0 = x\n","    x = self.ffn(x)\n","    x = self.dropout1(x)\n","    x = self.layer_norm2(x+x0)\n","    return x  "],"metadata":{"id":"erS3ID9d13f1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoder\n","\n","class Encoder(nn.Module):\n","  def __init__(self, enc_voc_size, max_len, d_model,\n","               ffn_hidden, n_head, n_layers, drop_rate, device):\n","    self.embedding = Transformer_Embedding(enc_voc_size, max_len, d_model, drop_rate)\n","    self.layers = nn.ModuleList([Encoder_Layer(d_model, ffn_hidden,\n","                                               n_head, drop_rate) for _ in range(n_layers)])\n","  def forward(self, x, src_mask):\n","    x = self.embedding(x)\n","    for layer in self.layers:\n","      x = layer(x, src_mask)\n","    return x"],"metadata":{"id":"xwqcmjsW3k1q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Decoder Layer"],"metadata":{"id":"I5tmU68-83s2"}},{"cell_type":"code","source":["# Decoder Layer\n","\n","class Decoder_Layer(nn.Module):\n","  def __init__(self, d_model, ffn_hidden, n_head, drop_rate):\n","    super(Decoder_Layer, self).__init__()\n","    self.self_attention = MultiHead_Attention(d_model, n_head)\n","    self.layer_norm1 = Layer_Normalization(d_model)\n","    self.dropout1 = nn.Dropout(drop_rate)\n","\n","    self.enc_dec_attention = MultiHead_Attention(d_model, n_head)\n","    self.layer_norm2 = Layer_Normalization(d_model)\n","    self.dropout2 = nn.Dropout(drop_rate)\n","\n","    self.ffn = Feed_Forward_Network(d_model, ffn_hidden, drop_rate)\n","    self.layer_norm3 = Layer_Normalization(d_model)\n","    self.dropout3 = nn.Dropout(drop_rate)\n","\n","  def forward(self, x_dec, x_enc, trg_mask, src_mask):\n","    x_dec0 = x_dec\n","    x_dec = self.self_attention(x_dec, x_dec, x_dec, trg_mask)\n","    x_dec = self.dropout1(x_dec)\n","    x = self.layer_norm1(x_dec + x_dec0)\n","\n","    if x_enc is not None:\n","      x0 = x\n","      x = self.enc_dec_attention(Q=x, K=x_enc, V=x_enc, mask=src_mask)\n","      x = self.dropout1(x)\n","      x = self.layer_norm1(x + x_dec0)  \n","\n","    x0 = x\n","    x = self.ffn(x)\n","    x = self.dropout3(x)\n","    x = self.layer_norm3(x+x0)\n","    return x   "],"metadata":{"id":"fK3Emk-P83CY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decoder\n","\n","class Decoder(nn.Module):\n","  def __init__(self, dec_voc_size, max_len, d_model,\n","               ffn_hidden, n_head, n_layers, drop_rate, device):\n","    super(Decoder, self).__init__()\n","    self.embedding = Transformer_Embedding(dec_voc_size, max_len, d_model, drop_rate)\n","    self.layers = nn.ModuleList([Decoder_Layer(d_model,\n","                                               ffn_hidden, n_head, drop_rate) for _ in range(n_layers)])\n","    self.linear = nn.Linear(d_model, dec_voc_size)\n","\n","  def forward(self, x_dec, x_enc, trg_mask, src_mask):\n","    x_dec = self.embedding(x_dec)\n","    for layer in self.layers:\n","      x_dec = layer(x_dec, x_enc, trg_mask, src_mask)\n","\n","    out = self.linear(x)\n","    return out"],"metadata":{"id":"DHMB4OR9_mKr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformer"],"metadata":{"id":"QztWmAd4E0Yy"}},{"cell_type":"code","source":["# Transformer\n","\n","class Transformer(nn.Module):\n","\n","    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n","                 ffn_hidden, n_layers, drop_prob, device):\n","        super().__init__()\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.trg_sos_idx = trg_sos_idx\n","        self.device = device\n","        self.encoder = Encoder(d_model=d_model,\n","                               n_head=n_head,\n","                               max_len=max_len,\n","                               ffn_hidden=ffn_hidden,\n","                               enc_voc_size=enc_voc_size,\n","                               drop_prob=drop_prob,\n","                               n_layers=n_layers,\n","                               device=device)\n","\n","        self.decoder = Decoder(d_model=d_model,\n","                               n_head=n_head,\n","                               max_len=max_len,\n","                               ffn_hidden=ffn_hidden,\n","                               dec_voc_size=dec_voc_size,\n","                               drop_prob=drop_prob,\n","                               n_layers=n_layers,\n","                               device=device)\n","\n","    def forward(self, src, trg):\n","        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n","\n","        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)\n","\n","        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * \\\n","                   self.make_no_peak_mask(trg, trg)\n","\n","        enc_src = self.encoder(src, src_mask)\n","        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n","        return output\n","\n","    def make_pad_mask(self, q, k, q_pad_idx, k_pad_idx):\n","        len_q, len_k = q.size(1), k.size(1)\n","\n","        # batch_size x 1 x 1 x len_k\n","        k = k.ne(k_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # batch_size x 1 x len_q x len_k\n","        k = k.repeat(1, 1, len_q, 1)\n","\n","        # batch_size x 1 x len_q x 1\n","        q = q.ne(q_pad_idx).unsqueeze(1).unsqueeze(3)\n","        # batch_size x 1 x len_q x len_k\n","        q = q.repeat(1, 1, 1, len_k)\n","\n","        mask = k & q\n","        return mask\n","\n","    def make_no_peak_mask(self, q, k):\n","        len_q, len_k = q.size(1), k.size(1)\n","\n","        # len_q x len_k\n","        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n","\n","        return mask"],"metadata":{"id":"nVanUDpzBCoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k = torch.tensor([[1,2],[3,4]])\n","idx = torch.tensor([[1,1],[1,1]])\n","k.ne(idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VLVyjjDJoC7O","executionInfo":{"status":"ok","timestamp":1676440341195,"user_tz":-540,"elapsed":6,"user":{"displayName":"강태욱","userId":"11730563988901671307"}},"outputId":"f7365ef8-c5bb-4d22-e75f-0fd8063f5bc8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True],\n","        [ True,  True]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[],"metadata":{"id":"FTir9lGkoW8C"},"execution_count":null,"outputs":[]}]}